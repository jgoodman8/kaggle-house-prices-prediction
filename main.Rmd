---
title: "Kaggle House Prices"
author: Javier Guzman Figueira Dominguez
date: Diciembre 2017
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```

Inspección de los datos
-----------------------

```{r, echo=FALSE}
required_packages <- c("ggplot2", "dplyr", "caret", "kernlab", "glmnet", "xgboost")
new_packages <- required_packages[!(required_packages %in% installed.packages()[, "Package"])]
if (length(new_packages)) install.packages(new_packages)
# library(pls)
library(ggplot2)
library(dplyr)
library(caret)
library(kernlab)
library(glmnet)
library(xgboost)

train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

Las dimensiones del conjunto de entrenamiento son las siguientes:

```{r}
dim(train)
dim(test)
```

A continuacin, procedemos a examinar las variables del dataset:

```{r}
str(train)
```

Y observamos el inicio:

```{r}
head(train)
```

Tratamiento de valores perdidos
-------------------------------

```{r}
numeric <- names(train)[which(sapply(train, is.numeric))] # Variables numéricas

lost_numeric_values_count <- colSums(sapply(train[, numeric], is.na))

plot_Missing <- function(data_in, title = NULL) {
    temp_df <- as.data.frame(ifelse(is.na(data_in), 0, 1))
    temp_df <- temp_df[, order(colSums(temp_df))]
    data_temp <- expand.grid(list(x = 1:nrow(temp_df), y = colnames(temp_df)))
    data_temp$m <- as.vector(as.matrix(temp_df))
    data_temp <- data.frame(x = unlist(data_temp$x), y = unlist(data_temp$y), m = unlist(data_temp$m))
    ggplot2::ggplot(data_temp) +
      ggplot2::geom_tile(ggplot2::aes(x = x, y = y, fill = factor(m))) +
      ggplot2::scale_fill_manual(values = c("white", "black"), name = "Missing\n(0=Yes, 1=No)") +
      ggplot2::theme_light() +
      ggplot2::ylab("") + ggplot2::xlab("") +
      ggplot2::ggtitle(title)
}

plot_Missing(train[, colSums(is.na(train)) > 0])
```

Predicion variable distribution
-------------------------------



```{r}
par(mfrow = c(1, 2))
hist(train$SalePrice, main = "SalePrice")
hist(log10(train$SalePrice), main = "Log10(SalePrice)")

par(mfrow = c(1, 2))
boxplot(train$SalePrice, main = "SalePrice")
boxplot(log10(train$SalePrice), main = "Log10(SalePrice)")
```

Los values (NA) count

```{r}
na_info <- apply(is.na(train), 2, sum)
lost_values_info <- which(na_info > 0)
filtered_lost_values_info <- na_info[lost_values_info]
df_filtered_lost_values_info <- data.frame(filtered_lost_values_info)

# TODO: Put dots instead of an histogram

ggplot() +
    geom_point(aes(x = rownames(df_filtered_lost_values_info), y = df_filtered_lost_values_info$filtered_lost_values_info)) +
    geom_hline(yintercept = nrow(train) * 0.05, color = 'pink') +
    geom_hline(yintercept = nrow(train) * 0.10, color = 'orange') +
    geom_hline(yintercept = nrow(train) * 0.20, color = 'blue') +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    xlab('Features with lost values') +
    ylab('Number of lost values')
```

- Distribución de los valores perdidos en función de la variable Log(SalePrice)

```{r}
for (feature in names(filtered_lost_values_info)) {
    categories <- train[, feature]
    print(ggplot(data = train, aes(x = feature, y = log(SalePrice), fill = categories)) +
        geom_boxplot())
}
```

- Examinamos la distribución de las variables continuas con respecto a Log(SalePrice)

```{r}
numeric_features <- numeric <- names(train)[which(sapply(train, is.numeric))]
numeric_feature_with_lost_values <- intersect(numeric_features, names(filtered_lost_values_info))
for (feature in numeric_feature_with_lost_values) {
    print(ggplot(data = train, aes(x = train[, feature], y = log(train$SalePrice))) +
          geom_point() +
          geom_smooth(method = "lm") +
          xlab(label = feature))
}
```


- GarageYrBlt
Se aprecia que es una propidad que, lógicamente, está muy relacionada con YearBuilt (año de construcción). En general, se puede decir que GarageYrBlt tiende a ser igual a YearBuilt. Por consiguiente, en los valores perdidos de GarageYrBlt, se procede a asígnar el correspondiente valor de YearBuilt.
```{r}
ggplot(data = train, aes(x = train$GarageYrBlt, y = train$YearBuilt)) +
          geom_point() +
          geom_smooth(method = "lm")

train$GarageYrBlt[is.na(train$GarageYrBlt)] <- train$YearBuilt[is.na(train$GarageYrBlt)]
```

- LotFrontage
Por lógica, se puede decir que el área de la propiedad con la longitud de la fachada. Para confirmarlo, comprobamos la correlación entre ellas:

```{r}
cor(train$LotFrontage, train$LotArea, use = "complete.obs")
cor(log(train$LotFrontage), log(train$LotArea), use = "complete.obs")
```

Y visualizamos su relación:

```{r}
ggplot(data = train, aes(x = train$LotArea, y = train$LotFrontage)) +
    geom_point() +
    geom_smooth(method = "lm")

ggplot(data = train, aes(x = log(train$LotArea), y = log(train$LotFrontage))) +
          geom_point() +
          geom_smooth(method = "lm")
```

Se puede confirmar que existe una alta correlación directa entre *LotFrontage* con *LotArea*. Dado, que estas dos propiedades están relacionadas, seguramente una de ellas sea desechada en el proceso de selección de variables. Idependientemente de ello, en este paso sustituiremos los valores de *LotFrontage*, por la mediana de los valores existentes.

```{r}
train$LotFrontage[is.na(train$LotFrontage)] <- median(train$LotFrontage[!is.na(train$LotFrontage)])
cor(train$LotFrontage, train$LotArea, use = "complete.obs")
cor(log(train$LotFrontage), log(train$LotArea), use = "complete.obs")
```

Observamos que la correlación continúa siendo similar después de tratar los valores perdidos en *LotFrontage*.

**TODO:** Quizás remplazar con la media no sea la mejor opción (cambia bastante la correlación). Si no se encuentra una solución mejor, quizás habría que cargarse directamente la variable.

```{r}
ggplot(data = train, aes(x = log(train$LotArea), y = log(train$LotFrontage))) +
          geom_point() +
          geom_smooth(method = "lm")
```

- MasVnrArea
Existe una gran cantidad de entradas con valor 0. Esto seguramente se deba a la carencia de "chapado":
```{r}
qplot(data = train, x = log(MasVnrArea), y = log(SalePrice), col = MasVnrType)
```

También observamos que en ambas variables los valores perdidos (8) forman parte de los mismos ejemplos:
```{r}
ifelse(train$Id[is.na(train$MasVnrArea)] == train$Id[is.na(train$MasVnrType)], "Equals", "Non equals")
```

Por consiguiente, se elimina la caraterística *MasVnrArea*, ya que las entradas con valor 0, por ser del tipo "None", hacen que la información desprendida de la variable esté "deformada".

```{r}
train <- dplyr::select(train, - MasVnrArea)
```

Ahora se deben tratar los valores perdidos de *MasVnrType*. Para ello, observamos *MasVnrType* en relación a *SalePrice* para entender su distribución.

```{r}
qplot(data = train, x = train$MasVnrType, y = log10(train$SalePrice), geom = c("boxplot"), fill = train$MasVnrType)
```

Asignamos a los valores perdidos el tipo "BrkFace", por mayor proximidad de sus medias. Aunque también se les podría asignar el tipo "Stone".

```{r}
train$MasVnrType[is.na(train$MasVnrType)] <- "BrkFace"
```

Finalmente, observamos la distribución resultante en las categorías de *MasVnrType* en relación a *SalePrice*.

```{r}
qplot(data = train, x = train$MasVnrType, y = log10(train$SalePrice), geom = c("boxplot"), fill = train$MasVnrType)
```

## Variables categóricas


Las siguientes caraterísticas con valores perdidos se corresponden con aquellas que contienen entradas en las que hay una ausencia de la propiedad a la que representan. Por ejemplo, la propiedad *PoolQC* representa la calidad de la piscina, pero es obvio que en aquellas propiedades en las haya una ausencia de piscina, será inviable representar su calidad. Por consiguiente, se le asignarán el tipo "None" a aquellos valores ausentes (NA). Las caracteríasticas que contienen este tipo de valores perdidos son:  *PoolQC*,   *MiscFeature*, *Alley*,  *Fence*, *FireplaceQu*, *GarageCond*, *GarageFinish*, *GarageQual*, *GarageType*, *BsmtCond*, *BsmtExposure*, *BsmtFinType1*, *BsmtFinType2* y *BsmtQual*.

```{r}
none_types <-
    c(
  "PoolQC",
  "MiscFeature",
  "Alley",
  "Fence",
  "FireplaceQu",
  "GarageCond",
  "GarageFinish",
  "GarageQual",
  "GarageType",
  "BsmtCond",
  "BsmtExposure",
  "BsmtFinType1",
  "BsmtFinType2",
  "BsmtQual"
  )

for (none in none_types) {
    if (is.factor(train[, none])) {
        train[, none] <- as.character(train[, none])
        train[, none][which(is.na(train[, none]))] <- "None"
        train[, none] <- factor(train[, none])
    } else {
        print(length(train[, none][is.na(train[, none])]))
        train[, none][is.na(train[, none])] <- "None"
    }
}
```

- Electrical
Esta característica presenta un valor perdido. Dada la mínima influencia que puede tener, se le asigna la categoría mayoritaria.

```{r}
train$Electrical[is.na(train$Electrical)] <- as.character(sort(train$Electrical, decreasing = TRUE)[1])
```

# Corrección de valores perdidos en el conjunto de test

En primer lugar, se examinan los valores perdidos que presenta el conjunto de test y se visualizan de la misma forma que se procedió con el conjunto de entrenamiento.

```{r}
na_info_test <- apply(is.na(test), 2, sum)
lost_values_info_test <- which(na_info_test > 0)
filtered_lost_values_info_test <- na_info_test[lost_values_info_test]
df_filtered_lost_values_info_test <- data.frame(filtered_lost_values_info_test)

# TODO: Change and add a histogram
ggplot() +
    geom_point(aes(x = rownames(df_filtered_lost_values_info_test), y = df_filtered_lost_values_info_test$filtered_lost_values_info_test)) +
    geom_hline(yintercept = nrow(test) * 0.05, color = 'pink') +
    geom_hline(yintercept = nrow(test) * 0.10, color = 'orange') +
    geom_hline(yintercept = nrow(test) * 0.20, color = 'blue') +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    xlab('Features with lost values') +
    ylab('Number of lost values')
```

En las caraterísticas que presenten valores perdidos y ya se haya examinado previamente, se proceden a tratar de la misma forma para realizar un procedimiento consistente.

```{r}
for (none in none_types) {
    if (is.factor(test[, none])) {
        test[, none] <- as.character(test[, none])
        test[, none][which(is.na(test[, none]))] <- "None"
        test[, none] <- factor(test[, none])
    } else {
        print(length(test[, none][is.na(test[, none])]))
        test[, none][is.na(test[, none])] <- "None"
    }
}

test$GarageYrBlt[is.na(test$GarageYrBlt)] <- test$YearBuilt[is.na(test$GarageYrBlt)]
test$LotFrontage[is.na(test$LotFrontage)] <- median(test$LotFrontage[!is.na(test$LotFrontage)])
test <- dplyr::select(test, - MasVnrArea)
test$MasVnrType[is.na(test$MasVnrType)] <- "Stone"
```

Así mismo, se procede a tratar las demás variables. Por una parte, se procesan las de tipo numérico, asignando la mediana a los valores faltantes.

```{r}
lost_test <- c("BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath", "GarageCars", "GarageArea")

for (lost in lost_test) {
    test[, lost][is.na(test[, lost])] <- median(test[, lost][!is.na(test[, lost])])
}
```

En cuanto a las variables de tipo nominal, se les asigna la moda de sus valores.

```{r}
lost_nominal_test <- c("Exterior1st", "Exterior2nd", "Functional", "KitchenQual", "MSZoning", "SaleType", "Utilities")

for (lost in lost_nominal_test) {
    test[, lost][is.na(test[, lost])] <- as.character(sort(test[, lost], decreasing = T)[1])
}
```

Antes de continuar, comprobamos que no hay valores perdidos en ninguno de los dos conjuntos.

```{r}
sum(is.na(train))
sum(is.na(test))
```

# Transformación de datos

En primer lugar, se procede a eliminar la propiedad Id de los conjuntos de entrenamiento y test.

```{r}
transformed_train <- dplyr::select(train, - Id)
transformed_test <- dplyr::select(test, - Id)
```

## Regularización de las variables continuas

Tal y como se ha mostrado, la variable *SalePrice* contiene una distribución asimétrica. Por consiguiente, para evitar el efecto que los valores extremos puedan causar, se procede a aplicar logaritmos a los valores de la distribución. 

Así mismo, se procede a mostrar diagramas de densidad de cada una de las características que contengan datos númericos. De esta forma se podrá observa que transformaciones pueden ser convenientes de hacer a cada variable.

```{r}
transformed_train$SalePrice <- log(transformed_train$SalePrice)

for (feature in names(transformed_train)) {
    if (is.numeric(transformed_train[, feature])) {
        print(ggplot2::ggplot(data = transformed_train, aes(x = transformed_train[, feature])) +
                      geom_density() +
                      xlab(feature))
    }
}
```


Let's normalize the continuous values
```{r}
features_to_center <- c("MSSubClass", "LotFrontage", "LotArea", "BsmtFinSF1", "BsmtUnfSF", "TotalBsmtSF", "X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", "GrLivArea", "BsmtFullBath", "BsmtHalfBath", "WoodDeckSF", "OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "PoolArea", "MiscVal")
feaures.to.log <- c("SalePrice","OverallQual", "OverallCond", "GareageArea", "MoSold")

train_pre_processed_values <- predict(caret::preProcess(transformed_train[, - ncol(transformed_train)], method = c("center", "scale")), transformed_train)
test_pre_processed_values <- predict(caret::preProcess(transformed_test, method = c("center", "scale")), transformed_test)
```

```{r}
transformed_train$GrLivArea <- log(transformed_train$GrLivArea)
for (feature in names(transformed_train)) {
    if (is.numeric(transformed_train[, feature])) {
        print(ggplot2::ggplot(data = transformed_train, aes(x = transformed_train[, feature])) +
                      geom_density() +
                      xlab(feature))
    }
}
```


## INTENTANDO SACAR ALGO DE AQUÍ: https://www.kaggle.com/bwboerman/r-data-table-glmnet-xgboost-with-caret

Creamos grupos de variables, según su tipo: variables de medidas, variables de recuento, variables de valor, categóricas.

```{r}
train.dt <- train
test.dt <- cbind(test, SalePrice=as.integer(NA))
full.dt <- rbindlist(list(train.dt, test.dt), use.names = F, fill = F)

variablesSquareFootage <- c(
  "LotFrontage",        ## Linear feet of street connected to property 
  "LotArea",            ## Lot size in square feet
  "BsmtFinSF1",       ## Type 1 finished square feet    
  "BsmtFinSF2",       ## Type 2 finished square feet
  "BsmtUnfSF",        ## Unfinished square feet of basement area
  "TotalBsmtSF",        ## Total square feet of basement area
  "X1stFlrSF",       ## First Floor square feet
  "X2ndFlrSF",      ## Second floor square feet
  "LowQualFinSF",   ## Low quality finished square feet (all floors)
  "GrLivArea",        ## Above grade (ground) living area square feet
  "GarageArea",     ## Size of garage in square feet
  "WoodDeckSF",     ## Wood deck area in square feet
  "OpenPorchSF",    ## Open porch area in square feet  
  "EnclosedPorch",  ## Enclosed porch area in square feet 
  "X3SsnPorch",  ## Three season porch area in square feet 
  "ScreenPorch",    ## Screen porch area in square feet
  "PoolArea"            ## Pool area in square feet
)

variablesCounts <- c(
  "BsmtFullBath",       ## Basement full bathrooms
  "BsmtHalfBath",       ## Basement half bathrooms
  "FullBath",             ## Full bathrooms above grade
  "HalfBath",             ## Half baths above grade
  "BedroomAbvGr",       ## Bedrooms above grade (does NOT include basement bedrooms)
  "KitchenAbvGr",       ## Kitchens above grade
  "TotRmsAbvGrd",       ## Total rooms above grade (does not include bathrooms)
  "Fireplaces",       ## Number of fireplaces
  "GarageCars"      ## Size of garage in car capacity
)

variablesValues <- c(
  "MiscVal",        ## $ Value of miscellaneous feature
  "SalePrice"       ## $ Price paid
)

variablesFactor <- colnames(full.dt)[which(as.vector(full.dt[,sapply(full.dt, class)]) == "character")]
variablesFactor <- setdiff(variablesFactor, "dataPartition") 
variablesFactor <- c(variablesFactor,
                     ## variables with data type integer which are factors
                     "MSSubClass",     ## Identifies the type of dwelling involved in the sale
                     "OverallQual",    ## Rates the overall material and finish of the house
                     "OverallCond"     ## Rates the overall condition of the house
)
```

- Skewed variables

```{r}
skewedVariables <- sapply(full.dt[, c(variablesSquareFootage,variablesValues), with = FALSE],
                          function(x){skew(x,na.rm=TRUE)}) ## including response variable
## keep only features that exceed a threshold for skewness
skewedVariables <- skewedVariables[skewedVariables > 0.50]
## transform excessively skewed features with log1p
skewedVariables <- names(skewedVariables)
full.dt[, (skewedVariables) := lapply(.SD, function(x) log1p(x)), .SDcols = skewedVariables]
```

- Scale....

```{r}
varScale <- setdiff(c(variablesSquareFootage, variablesValues), c("SalePrice")) ## Do not scale response
full.dt[, (varScale) := lapply(.SD, function(x) scale(x, center = T, scale = T)), .SDcols = varScale]
```

- Ordinal factors

```{r}
changeColType <- setDT(data.frame(sapply(full.dt,is.ordered)), keep.rownames = TRUE)[sapply.full.dt..is.ordered.==TRUE]$rn
full.dt[,(changeColType):= lapply(.SD, as.integer), .SDcols = changeColType]
```

- Spliting into train and test

```{r}
train.dt.processed <- full.dt[1:nrow(train.dt),]
test.dt.processed <- full.dt[1:nrow(test.dt),]
test.dt.processed <- dplyr::select(test.dt.processed, - SalePrice)
```

### Selección de variables

- PCA

```{r}
pca_train.processed <- predict(caret::preProcess(train_pre_processed_values[, - ncol(transformed_train)], method = c("pca")), newdata = train_pre_processed_values)
pca_test.processed <- predict(caret::preProcess(test_pre_processed_values, method = c("pca")), newdata = test_pre_processed_values)

qplot(train_pre_processed_values)
```

- lm

```{r}
exploratory_lm = lm(SalePrice ~ ., data = train_pre_processed_values)
summary(exploratory_lm)

# step_forward_stats <- olsrr::ols_step_forward(exploratory_lmstep_forward_stats)
# plot(step_forward_stats)
```

```{r}
train_set_selected <-transformed_train[, c("OverallQual", "GrLivArea", "YearBuilt", "OverallCond", "GarageCars", "TotalBsmtSF", "RoofMatl", "BsmtFinSF1", "Condition2", "LotFrontage", "SaleCondition", "Foundation", "KitchenAbvGr", "MSZoning", "SalePrice")]

# selected_features <- c(step_forward_stats$predictors, "SalePrice")

# names(train_set) <- selected_features
```

## Entrenamiento

```{r}
set.seed(12345)
model <- caret::train(SalePrice ~ .,
  data = train.dt.processed,
  method = 'xgbLinear', # svmRadial # xgbLinear # glmnet #svmLinear2
  trControl = caret::trainControl(
  method = 'repeatedcv',
  number = 10,
  repeats = 5
  ))

model
```

- Entrenamiento sofisticado

```{r}
set.seed(12345)

features <- setdiff(names(full.dt), c("SalePrice", "Id", "dataPartition"))

treatplan <- vtreat::designTreatmentsZ(train.dt.processed, minFraction = 0.01, rareCount = 0, features, verbose = FALSE)
train.full.treat <- prepare(treatplan, dframe = train.dt.processed, codeRestriction = c("clean", "lev"))
test.treat  <- prepare(treatplan, dframe = test.dt.processed, codeRestriction = c("clean", "lev"))

trControl <- trainControl(
        method="cv",
        number=7,
        savePredictions="final",
        index=createResample(train.dt.processed$OverallQual, 7),  
        allowParallel =TRUE
)
xgbTreeGrid <- expand.grid(nrounds = 400, max_depth = 3, eta = 0.1, gamma = 0, colsample_bytree = 1.0,  subsample = 1.0, min_child_weight = 4)
glmnetGridElastic <- expand.grid(.alpha = 0.3, .lambda = 0.009) ## notice the . before the parameter
glmnetGridLasso <- expand.grid(.alpha = 1, .lambda = seq(0.001,0.1,by = 0.001))
glmnetGridRidge <- expand.grid(.alpha = 0, .lambda = seq(0.001,0.1,by = 0.001))
set.seed(333)
modelList <<- caretEnsemble::caretList(
                  x = train.full.treat,
                  y = train.dt.processed$SalePrice,
                  trControl=trControl,
                  metric="RMSE",
                  tuneList=list(
                  ## Do not use custom names in list. Will give prediction error with greedy ensemble. Bug in caret.
                          xgbTree = caretEnsemble::caretModelSpec(method="xgbTree",  tuneGrid = xgbTreeGrid, nthread = 8),
                          #glmnet=caretModelSpec(method="glmnet", tuneGrid = glmnetGridElastic), ## Elastic, highly correlated with lasso and ridge regressions
                          glmnet=caretEnsemble::caretModelSpec(method="glmnet", tuneGrid = glmnetGridLasso), ## Lasso
                          glmnet=caretEnsemble::caretModelSpec(method="glmnet", tuneGrid = glmnetGridRidge) ## Ridge
                          #svmLinear3= caretModelSpec(method="svmLinear3", tuneLenght = 20) ## SVM 
                          )
)

greedyEnsemble <- caretEnsemble(
  modelList, 
  metric="RMSE",
  trControl=trainControl(
    number=7, method = "cv"
  ))

summary(greedyEnsemble)
```

- Model evaluation

```{r} 
train_predict <- stats::predict(model, train.dt.processed)
ggplot2::qplot(x = exp(train_predict), y = exp(train.dt.processed$SalePrice),
               geom = c("point", "smooth"), method = "lm",
               xlab = "Paredicted", ylab = "Real")
```

- Variable importance

```{r}
model.variable.importance <- caret::varImp(model)
ggplot2::ggplot(model.variable.importance) + geom_density()
```

```{r}
best.predictor.importance.index <- model.variable.importance$importance %>% mutate(names = row.names(.)) %>% arrange(-Overall)

best.predictor.importance.variables <- c("SalePrice", best.predictor.importance.index$names[1:10])
```

Trying to improve model ...

```{r}
best.predictor.importance.data <- train_pre_processed_values[, best.predictor.importance.variables]

model <- caret::train(SalePrice ~ .,
  data = best.predictor.importance.data,
  method = 'xgbLinear', # svmRadial # xgbLinear # glmnet #svmLinear2
  trControl = caret::trainControl(
  method = 'repeatedcv',
  number = 10,
  repeats = 5
  ))

model
```

## Prediction

```{r}
t <- select(test.dt.processed, - Id)
test_predict <- stats::predict(model, test.dt.processed)
```

## Submiting data

```{r}
prediction.table <- data.frame(Id = test$Id, SalePrice = exp(test_predict))
write.csv(prediction.table, paste0('xgbLinear_top10variables', "_predictions", ".csv"), row.names = F)
```




