---
title: "Kaggle House Prices"
author: Javier Guzman Figueira Dominguez
date: Diciembre 2017
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```

Inspección de los datos
-----------------------

```{r, echo=FALSE}
required_packages <- c("ggplot2", "dplyr", "caret", "kernlab", "glmnet", "xgboost", "data.table", "Metrics", "cowplot")
new_packages <- required_packages[!(required_packages %in% installed.packages()[, "Package"])]
if (length(new_packages)) install.packages(new_packages)

library(ggplot2)
library(dplyr)
library(caret)
library(kernlab)
library(glmnet)
library(xgboost)
library(data.table)
library(Metrics)
library(cowplot)

train <- read.csv("train.csv")
test <- read.csv("test.csv")
full.set <- data.frame(
                       data.table::rbindlist(list(train, cbind(test, SalePrice = as.integer(NA))),
                                             use.names = F,
                                             fill = F))
```

```{r}
getTrainFromFullSet <- function() {
    return(full.set[c(1:nrow(train)),])
}
```

Las dimensiones del conjunto de entrenamiento son las siguientes:

```{r}
dim(train)
dim(test)
dim(full.set)
```

A continuacin, procedemos a examinar las variables del dataset:

```{r}
str(full.set)
```

Y observamos el inicio:

```{r}
head(full.set)
```

- Análisis de valores perdidos

```{r}
lost.count <- apply(is.na(select(full.set, - SalePrice)), 2, sum)
lost.count <- which(lost.count > 0)
lost.percentage <- (lost.count / nrow(full.set)) * 100

data.frame(lost.count, lost.percentage)
```

```{r} # TODO: Clarificar o simplificar operaciones
numeric <- names(full.set)[which(sapply(train, is.numeric))] # Variables numéricas

lost_numeric_values_count <- colSums(sapply(full.set[, numeric], is.na))

data_in <- full.set[, colSums(is.na(select(full.set, - SalePrice))) > 0]

temp_df <- as.data.frame(ifelse(is.na(data_in), 0, 1))
temp_df <- temp_df[, order(colSums(temp_df))]
data_temp <- expand.grid(list(x = 1:nrow(temp_df), y = colnames(temp_df)))
data_temp$m <- as.vector(as.matrix(temp_df))
data_temp <- data.frame(x = unlist(data_temp$x), y = unlist(data_temp$y), m = unlist(data_temp$m))

ggplot2::ggplot(data_temp) +
      ggplot2::geom_tile(ggplot2::aes(x = x, y = y, fill = factor(m))) +
      ggplot2::scale_fill_manual(values = c("white", "black"), name = "Perdido\n(0=Yes, 1=No)") +
      ggplot2::theme_light() +
      ggplot2::ylab("") + ggplot2::xlab("") +
      ggplot2::ggtitle("Valores perdidos en el conjunto total de datos")
```

- Análisis de la distribución de la variable clase

```{r}
par(mfrow = c(2, 2))

hist(train$SalePrice, main = "SalePrice")
hist(log10(train$SalePrice), main = "Log10(SalePrice)")

boxplot(train$SalePrice, main = "SalePrice")
boxplot(log10(train$SalePrice), main = "Log10(SalePrice)")

par(mfrow = c(1, 1))
```

- Distribución de los valores perdidos en función de la variable Log(SalePrice)

```{r}
for (feature in names(filtered_lost_values_info)) {
    categories <- train[, feature]
    print(ggplot(data = train, aes(x = feature, y = log(SalePrice), fill = categories)) +
        geom_boxplot())
}
```

- Examinamos la distribución de las variables continuas con respecto a Log(SalePrice)

```{r}
numeric_features <- numeric <- names(train)[which(sapply(train, is.numeric))]
numeric_feature_with_lost_values <- intersect(numeric_features, names(filtered_lost_values_info))

plots <- lapply(numeric_feature_with_lost_values, function(feature) {
    ggplot(data = train, aes(x = train[, feature], y = log(train$SalePrice))) +
          geom_point() +
          geom_smooth(method = "lm") +
          xlab(label = feature) +
          ylab(label = "Log(SalePrice)")
})

cowplot::plot_grid(plotlist = plots, ncol = 2)
```


## Tratamiento de los valores perdidos

- GarageYrBlt

Se aprecia que es una propidad que, lógicamente, está muy relacionada con YearBuilt (año de construcción). En general, se puede decir que GarageYrBlt tiende a ser igual a YearBuilt. Por consiguiente, en los valores perdidos de GarageYrBlt, se procede a asígnar el correspondiente valor de YearBuilt.

```{r}
ggplot(data = full.set, aes(x = GarageYrBlt, y = YearBuilt)) +
          geom_point() +
          geom_smooth(method = "lm")

full.set$GarageYrBlt[full.set$GarageYrBlt == 2207] <- 2007
full.set$GarageYrBlt[is.na(full.set$GarageYrBlt)] <- full.set$YearBuilt[is.na(full.set$GarageYrBlt)]
```

- LotFrontage

Por lógica, se puede decir que el área de la propiedad con la longitud de la fachada. Para confirmarlo, comprobamos la correlación entre ellas:

```{r}
cor(train$LotFrontage, train$LotArea, use = "complete.obs")
cor(log(train$LotFrontage), log(train$LotArea), use = "complete.obs")
```

Y visualizamos su relación:

```{r}
ggplot(data = train, aes(x = train$LotArea, y = train$LotFrontage)) +
    geom_point() +
    geom_smooth(method = "lm")

ggplot(data = train, aes(x = log(train$LotArea), y = log(train$LotFrontage))) +
          geom_point() +
          geom_smooth(method = "lm")
```

Se puede confirmar que existe una alta correlación directa entre *LotFrontage* con *LotArea*. Dado, que estas dos propiedades están relacionadas, seguramente una de ellas sea desechada en el proceso de selección de variables. Idependientemente de ello, en este paso sustituiremos los valores de *LotFrontage*, por la mediana de los valores existentes.

```{r}
train$LotFrontage[is.na(train$LotFrontage)] <- median(train$LotFrontage[!is.na(train$LotFrontage)])
cor(train$LotFrontage, train$LotArea, use = "complete.obs")
cor(log(train$LotFrontage), log(train$LotArea), use = "complete.obs")
```

Observamos que la correlación continúa siendo similar después de tratar los valores perdidos en *LotFrontage*.

**TODO:** Quizás remplazar con la media no sea la mejor opción (cambia bastante la correlación). Si no se encuentra una solución mejor, quizás habría que cargarse directamente la variable.

```{r}
ggplot(data = train, aes(x = log(train$LotArea), y = log(train$LotFrontage))) +
          geom_point() +
          geom_smooth(method = "lm")
```

- MasVnrArea
Existe una gran cantidad de entradas con valor 0. Esto seguramente se deba a la carencia de "chapado":
```{r}
qplot(data = train, x = log(MasVnrArea), y = log(SalePrice), col = MasVnrType)
```

También observamos que en ambas variables los valores perdidos (8) forman parte de los mismos ejemplos:
```{r}
ifelse(train$Id[is.na(train$MasVnrArea)] == train$Id[is.na(train$MasVnrType)], "Equals", "Non equals")
```

Por consiguiente, se elimina la caraterística *MasVnrArea*, ya que las entradas con valor 0, por ser del tipo "None", hacen que la información desprendida de la variable esté "deformada".

```{r}
train <- dplyr::select(train, - MasVnrArea)
```

Ahora se deben tratar los valores perdidos de *MasVnrType*. Para ello, observamos *MasVnrType* en relación a *SalePrice* para entender su distribución.

```{r}
qplot(data = train, x = train$MasVnrType, y = log10(train$SalePrice), geom = c("boxplot"), fill = train$MasVnrType)
```

Asignamos a los valores perdidos el tipo "BrkFace", por mayor proximidad de sus medias. Aunque también se les podría asignar el tipo "Stone".

```{r}
train$MasVnrType[is.na(train$MasVnrType)] <- "BrkFace"
```

Finalmente, observamos la distribución resultante en las categorías de *MasVnrType* en relación a *SalePrice*.

```{r}
qplot(data = train, x = train$MasVnrType, y = log10(train$SalePrice), geom = c("boxplot"), fill = train$MasVnrType)
```

## Variables categóricas


Las siguientes caraterísticas con valores perdidos se corresponden con aquellas que contienen entradas en las que hay una ausencia de la propiedad a la que representan. Por ejemplo, la propiedad *PoolQC* representa la calidad de la piscina, pero es obvio que en aquellas propiedades en las haya una ausencia de piscina, será inviable representar su calidad. Por consiguiente, se le asignarán el tipo "None" a aquellos valores ausentes (NA). Las caracteríasticas que contienen este tipo de valores perdidos son:  *PoolQC*,   *MiscFeature*, *Alley*,  *Fence*, *FireplaceQu*, *GarageCond*, *GarageFinish*, *GarageQual*, *GarageType*, *BsmtCond*, *BsmtExposure*, *BsmtFinType1*, *BsmtFinType2* y *BsmtQual*.

```{r}
none_types <-
    c(
  "PoolQC",
  "MiscFeature",
  "Alley",
  "Fence",
  "FireplaceQu",
  "GarageCond",
  "GarageFinish",
  "GarageQual",
  "GarageType",
  "BsmtCond",
  "BsmtExposure",
  "BsmtFinType1",
  "BsmtFinType2",
  "BsmtQual"
  )

for (none in none_types) {
    if (is.factor(train[, none])) {
        train[, none] <- as.character(train[, none])
        train[, none][which(is.na(train[, none]))] <- "None"
        train[, none] <- factor(train[, none])
    } else {
        print(length(train[, none][is.na(train[, none])]))
        train[, none][is.na(train[, none])] <- "None"
    }
}
```

- Electrical
Esta característica presenta un valor perdido. Dada la mínima influencia que puede tener, se le asigna la categoría mayoritaria.

```{r}
train$Electrical[is.na(train$Electrical)] <- as.character(sort(train$Electrical, decreasing = TRUE)[1])
```

# Corrección de valores perdidos en el conjunto de test

En primer lugar, se examinan los valores perdidos que presenta el conjunto de test y se visualizan de la misma forma que se procedió con el conjunto de entrenamiento.

```{r}
na_info_test <- apply(is.na(test), 2, sum)
lost_values_info_test <- which(na_info_test > 0)
filtered_lost_values_info_test <- na_info_test[lost_values_info_test]
df_filtered_lost_values_info_test <- data.frame(filtered_lost_values_info_test)

# TODO: Change and add a histogram
ggplot() +
    geom_point(aes(x = rownames(df_filtered_lost_values_info_test), y = df_filtered_lost_values_info_test$filtered_lost_values_info_test)) +
    geom_hline(yintercept = nrow(test) * 0.05, color = 'pink') +
    geom_hline(yintercept = nrow(test) * 0.10, color = 'orange') +
    geom_hline(yintercept = nrow(test) * 0.20, color = 'blue') +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    xlab('Features with lost values') +
    ylab('Number of lost values')
```

En las caraterísticas que presenten valores perdidos y ya se haya examinado previamente, se proceden a tratar de la misma forma para realizar un procedimiento consistente.

```{r}
for (none in none_types) {
    if (is.factor(test[, none])) {
        test[, none] <- as.character(test[, none])
        test[, none][which(is.na(test[, none]))] <- "None"
        test[, none] <- factor(test[, none])
    } else {
        print(length(test[, none][is.na(test[, none])]))
        test[, none][is.na(test[, none])] <- "None"
    }
}

test$GarageYrBlt[is.na(test$GarageYrBlt)] <- test$YearBuilt[is.na(test$GarageYrBlt)]
test$LotFrontage[is.na(test$LotFrontage)] <- median(test$LotFrontage[!is.na(test$LotFrontage)])
test <- dplyr::select(test, - MasVnrArea)
test$MasVnrType[is.na(test$MasVnrType)] <- "Stone"
```

Así mismo, se procede a tratar las demás variables. Por una parte, se procesan las de tipo numérico, asignando la mediana a los valores faltantes.

```{r}
lost_test <- c("BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath", "GarageCars", "GarageArea")

for (lost in lost_test) {
    test[, lost][is.na(test[, lost])] <- median(test[, lost][!is.na(test[, lost])])
}
```

En cuanto a las variables de tipo nominal, se les asigna la moda de sus valores.

```{r}
lost_nominal_test <- c("Exterior1st", "Exterior2nd", "Functional", "KitchenQual", "MSZoning", "SaleType", "Utilities")

for (lost in lost_nominal_test) {
    test[, lost][is.na(test[, lost])] <- as.character(sort(test[, lost], decreasing = T)[1])
}
```

Antes de continuar, comprobamos que no hay valores perdidos en ninguno de los dos conjuntos.

```{r}
sum(is.na(train))
sum(is.na(test))
```

# Transformación de datos

En primer lugar, se procede a eliminar la propiedad Id de los conjuntos de entrenamiento y test.

```{r}
transformed_train <- dplyr::select(train, - Id)
transformed_test <- dplyr::select(test, - Id)
```

# Tratamiento de outliers

```{r}
ggplot(transformed_train, aes(y = SalePrice, x = GrLivArea)) + ggtitle("With Outliers") + geom_point()
ggplot(transformed_train, aes(y = log(SalePrice), x = log(GrLivArea))) + ggtitle("With Outliers (Log())") + geom_point()

transformed_train[transformed_train$GrLivArea > 4000,]$GrLivArea <- mean(transformed_train$GrLivArea) %>% as.numeric
ggplot(transformed_train, aes(y = SalePrice, x = GrLivArea)) + ggtitle("Without Outliers") + geom_point()
```


```{r} # TODO: Delete due move
train.dt <- transformed_train
test.dt <- cbind(transformed_test, SalePrice = as.integer(NA))
full.dt <- data.table::rbindlist(list(train.dt, test.dt), use.names = F, fill = F)
```

- Creación de una nueva variable: Area total basement xd

```{r}
full.dt$TotalSF = full.dt$TotalBsmtSF + full.dt$X1stFlrSF + full.dt$X2ndFlrSF
```

## Regularización de las variables continuas

Tal y como se ha mostrado, la variable *SalePrice* contiene una distribución asimétrica. Por consiguiente, para evitar el efecto que los valores extremos puedan causar, se procede a aplicar logaritmos a los valores de la distribución. 

Así mismo, se procede a mostrar diagramas de densidad de cada una de las características que contengan datos númericos. De esta forma se podrá observa que transformaciones pueden ser convenientes de hacer a cada variable.

```{r}
full.dt$SalePrice <- log(full.dt$SalePrice) # TODO: CHECK THIS :S

for (feature in names(transformed_train)) {
    if (is.numeric(transformed_train[, feature])) {
        print(ggplot2::ggplot(data = transformed_train, aes(x = transformed_train[, feature])) +
                      geom_density() +
                      xlab(feature))
    }
}
```


Let's normalize the continuous values
```{r}
variablesSquareFootage <- c(
  "LotFrontage", ## Linear feet of street connected to property 
  "LotArea", ## Lot size in square feet
  "BsmtFinSF1", ## Type 1 finished square feet    
  "BsmtFinSF2", ## Type 2 finished square feet
  "BsmtUnfSF", ## Unfinished square feet of basement area
  "TotalBsmtSF", ## Total square feet of basement area
  "X1stFlrSF", ## First Floor square feet
  "X2ndFlrSF", ## Second floor square feet
  "LowQualFinSF", ## Low quality finished square feet (all floors)
  "GrLivArea", ## Above grade (ground) living area square feet
  "GarageArea", ## Size of garage in square feet
  "WoodDeckSF", ## Wood deck area in square feet
  "OpenPorchSF", ## Open porch area in square feet  
  "EnclosedPorch", ## Enclosed porch area in square feet 
  "X3SsnPorch", ## Three season porch area in square feet 
  "ScreenPorch", ## Screen porch area in square feet
  "PoolArea" ## Pool area in square feet
)

full.dt[, variablesSquareFootage] <- log(1 + full.dt[, variablesSquareFootage, with = FALSE])
```

```{r}
full.dt <- data.frame(full.dt)
for (i in 1:ncol(full.dt)) {
    if (is.factor(full.dt[, i])) {
        levels(full.dt[, i]) <- c(1:length(levels(full.dt[, i])))
        full.dt[, i] <- as.numeric(full.dt[, i])
    }
}

feature_classes <- sapply(names(full.dt), function(x) {
    class(full.dt[, x])
})
numeric_feats <- names(feature_classes[feature_classes != "character"])
skewed_feats <- sapply(numeric_feats, function(x) {
    e1071::skewness(full.dt[, x], na.rm = TRUE)
})
skewed_feats <- skewed_feats[abs(skewed_feats) > 0.75]
for (x in names(skewed_feats)) {
    bc = caret::BoxCoxTrans(full.dt[, x], lambda = 0.15)
    full.dt[, x] = predict(bc, full.dt[, x])
}

full.dt.processed <- predict(caret::preProcess(select(full.dt, - SalePrice),
    method = c("center", "scale")), full.dt)
```

- Spliting into train and test

```{r}
train.dt.processed <- full.dt.processed[1:nrow(train.dt),]
test.dt.processed <- full.dt.processed[(nrow(train.dt) + 1):nrow(full.dt.processed),]
test.dt.processed <- dplyr::select(test.dt.processed, - SalePrice)
```

## Entrenamiento

```{r}
in_train <- createDataPartition(train.dt.processed$SalePrice, p = 0.7, list = F)
partial.train <- train.dt.processed[in_train,]
validation <- train.dt.processed[-in_train,]
```

- Entrenamiento "parcial"

```{r}
set.seed(12345)
model <- caret::train(SalePrice ~ .,
  data = partial.train,
  method = 'gbm', # svmRadial # xgbLinear # glmnet #svmLinear2
  trControl = caret::trainControl(
  method = 'repeatedcv',
  number = 10,
  repeats = 1
  ))

model
```

- Validación del modelo

```{r}
validation.prediction <- stats::predict(model, validation)
ggplot2::qplot(x = exp(validation.prediction), y = exp(validation$SalePrice),
               geom = c("point", "smooth"), method = "lm",
               xlab = "Paredicted", ylab = "Real")

rmse(validation$SalePrice, validation.prediction)
```

```{r}
rediduals <- (validation$SalePrice - validation.prediction)
qplot(validation$SalePrice - validation.prediction,)
```

- Variable importance

```{r} # SKIP
model.variable.importance <- caret::varImp(model)
ggplot2::ggplot(model.variable.importance) + geom_density()
```

```{r} # SKIP
best.predictor.importance.index <- model.variable.importance$importance %>% mutate(names = row.names(.)) %>% arrange(-Overall)

best.predictor.importance.variables <- c("SalePrice", best.predictor.importance.index$names[1:10])
```

Trying to improve model ...

```{r}
# best.predictor.importance.data <- train_pre_processed_values[, best.predictor.importance.variables]

model <- caret::train(SalePrice ~ .,
  data = train.dt.processed,
  method = 'gbm', # svmRadial # xgbLinear # glmnet #svmLinear2
  trControl = caret::trainControl(
  method = 'repeatedcv',
  number = 10,
  repeats = 1
  ))

model
```

## Prediction

```{r}
# t <- select(test.dt.processed, - Id)
test.prediction <- stats::predict(model, test.dt.processed)
```

## Submiting data

```{r}
prediction.table <- data.frame(Id = test$Id, SalePrice = exp(test.prediction))
write.csv(prediction.table, paste0('with_preValidation', "_predictions_1", ".csv"), row.names = F)
```


######## MORE ##################

```{r}
library(vtreat)
library(caretEnsemble)

features <- setdiff(names(full.dt), c("SalePrice", "Id", "dataPartition")) #Check in original code
treatplan <- vtreat::designTreatmentsZ(train.dt.processed, minFraction = 0.01, rareCount = 0, features, verbose = FALSE)
train.full.treat <- prepare(treatplan, dframe = train.dt.processed, codeRestriction = c("clean", "lev"))
test.treat <- prepare(treatplan, dframe = test.dt.processed, codeRestriction = c("clean", "lev"))

trControl <- trainControl(
        method = "cv",
        number = 7,
        savePredictions = "final",
        index = createResample(train.dt.processed$OverallQual, 7),
        allowParallel = TRUE
)
xgbTreeGrid <- expand.grid(nrounds = 400, max_depth = 3, eta = 0.1, gamma = 0, colsample_bytree = 1.0, subsample = 1.0, min_child_weight = 4)
glmnetGridElastic <- expand.grid(.alpha = 0.3, .lambda = 0.009) ## notice the . before the parameter
glmnetGridLasso <- expand.grid(.alpha = 1, .lambda = seq(0.001, 0.1, by = 0.001))
glmnetGridRidge <- expand.grid(.alpha = 0, .lambda = seq(0.001, 0.1, by = 0.001))
```

```{r}
# Write a function that gets a data set and converts all factor variables to dummy variables
convert.to.dummy <- function(data.set) {
    data.set <- data.frame(data.set)
    cat.var <- NULL
    temp.data <- data.frame(1:nrow(data.set))
    for (i in 1:ncol(data.set)) {
        if (class(data.set[, i]) == "factor") {
            cat.var <- c(cat.var, i)
            factor.levels <- levels(data.set[, i]) # Try to find a way to classify NA's as "NO" otherwise they generate problem downstream
            # First check if there is any 'NA-level'
            if (any(is.na(data.set[, i]))) {
                dummy.vector = ifelse(is.na(data.set[, i]), 1, 0)
                dummy.vector <- data.frame(dummy.vector)
                colnames(dummy.vector)[1] = paste("NO", names((data.set)[i]), sep = ".")
                temp.data <- cbind(temp.data, dummy.vector)
            }

            for (j in seq_along(factor.levels)) {
                # Then deal with normal factor levels
                dummy.vector = ifelse(data.set[, i] == factor.levels[j], 1, 0)

                #Since we already dealt with NAs above
                if (any(is.na(dummy.vector))) { dummy.vector[is.na(dummy.vector)] <- 0 }

                dummy.vector <- data.frame(dummy.vector)
                colnames(dummy.vector)[1] = paste(names((data.set)[i]),
                                                                  factor.levels[j], sep = ".")
                temp.data <- cbind(temp.data, dummy.vector)
            }
        }
    }
    #Remove the original categorical variables from data.set
    data.set <- data.set[, - cat.var]
    #Add the dummy.variable set
    temp.data <- temp.data[, -1] # remove the unnecessary column
    data.set <- cbind(data.set, temp.data)

    return(data.set)
}

training.processed.dummy <- convert.to.dummy(train.dt.processed)
test.dt.processed.dummy <- convert.to.dummy(test.dt.processed)
```

```{r}
set.seed(333)
modelList <<- caretEnsemble::caretList(
                  SalePrice ~ .,
                  data = train.dt.processed,
                  trControl = trControl,
                  metric = "RMSE",
                  tuneList = list(
    gbm = caretModelSpec(method = "gbm", tuneGrid = expand.grid(n.trees = 700, interaction.depth = 5,
                                                          shrinkage = 0.05, n.minobsinnode = 10)),
    xgbTree = caretModelSpec(method = "xgbTree", tuneGrid = expand.grid(nrounds = 2500, max_depth = 6, min_child_weight = 1.41,
                                                                  eta = 0.01, gamma = 0.0468, subsample = 0.769,
                                                                  colsample_bytree = 0.283))
## Do not use custom names in list. Will give prediction error with greedy ensemble. Bug in caret.
#xgbTree = caretModelSpec(method = "xgbTree", tuneGrid = xgbTreeGrid, nthread = 8),
#glmnet=caretModelSpec(method="glmnet", tuneGrid = glmnetGridElastic), ## Elastic, highly correlated with lasso and ridge regressions
# glmnet = caretModelSpec(method = "glmnet", tuneGrid = glmnetGridLasso), ## Lasso
# glmnet = caretModelSpec(method = "glmnet", tuneGrid = glmnetGridRidge) ## Ridge
#svmLinear3= caretModelSpec(method="svmLinear3", tuneLenght = 20) ## SVM 
                          )
)

greedy_ensemble <- caretEnsemble(modelList, metric = "RMSE", trControl = trainControl(number = 25))
```

```{r}
predictions <- predict(greedy_ensemble, newdata = test.dt.processed)
prediction.table <- data.frame(Id = test$Id, SalePrice = exp(predictions))
write.csv(prediction.table, paste0('complex', "_predictions_3", ".csv"), row.names = F)
```

