---
title: "Kaggle House Prices"
author: Javier Guzmán Figueira Dominguez
date: Febrero 2018
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```

# Inspección de los datos

```{r, echo=FALSE}
required_packages <- c("ggplot2", "dplyr", "caret", "kernlab", "glmnet", "xgboost", "data.table", "Metrics", "cowplot", "caretEnsemble", "dummies")
new_packages <- required_packages[!(required_packages %in% installed.packages()[, "Package"])]
if (length(new_packages)) install.packages(new_packages)

library(ggplot2)
library(dplyr)
library(caret)
library(kernlab)
library(glmnet)
library(xgboost)
library(data.table)
library(Metrics)
library(cowplot)
library(caretEnsemble)
library(dummies)

set.seed(333)

train <- read.csv("train.csv")
test <- read.csv("test.csv")
full.set <- data.frame(
                       data.table::rbindlist(list(train, cbind(test, SalePrice = as.integer(NA))),
                                             use.names = F,
                                             fill = F))
```

```{r}
updatePartitions <- function() {
    train.size <- nrow(train)
    full.set.size <- nrow(full.set)

    train <<- full.set[c(1:train.size),]
    test <<- full.set[c((train.size + 1):full.set.size),]
}
```

Las dimensiones del conjunto de entrenamiento son las siguientes:

```{r}
dim(train)
dim(test)
dim(full.set)
```

A continuacin, procedemos a examinar las variables del dataset:

```{r}
str(full.set)
```

Y observamos el inicio:

```{r}
head(full.set)
```

- Análisis de valores perdidos

```{r}
getLostValuesStats <- function() {
    lost.count <- colSums(sapply(select(full.set, - SalePrice), is.na))
    lost.count <- subset(lost.count, lost.count > 0)
    lost.percentage <- (lost.count / nrow(full.set)) * 100

    return(data.frame(lost.count, lost.percentage))
}

getLostValuesStats()
```

```{r}
lost.values.count <- full.set[, colSums(is.na(select(full.set, - SalePrice))) > 0]

is.lost.value <- as.data.frame(ifelse(is.na(lost.values.count), 0, 1))
is.lost.value <- is.lost.value[, order(colSums(is.lost.value))]

is.lost.value.grid <- expand.grid(list(x = 1:nrow(is.lost.value), y = colnames(is.lost.value)))
is.lost.value.grid$m <- as.vector(as.matrix(is.lost.value))
is.lost.value.grid <- data.frame(x = unlist(is.lost.value.grid$x), y = unlist(is.lost.value.grid$y), m = unlist(is.lost.value.grid$m))

ggplot2::ggplot(is.lost.value.grid) +
      ggplot2::geom_tile(ggplot2::aes(x = x, y = y, fill = factor(m))) +
      ggplot2::scale_fill_manual(values = c("white", "black"), name = "Perdido\n(0=Yes, 1=No)") +
      ggplot2::theme_light() +
      ggplot2::ylab("") + ggplot2::xlab("") +
      ggplot2::ggtitle("Valores perdidos en el conjunto total de datos")
```

- Análisis de la distribución de la variable clase

```{r}
par(mfrow = c(2, 2))

hist(train$SalePrice, main = "SalePrice")
hist(log10(train$SalePrice), main = "Log10(SalePrice)")

boxplot(train$SalePrice, main = "SalePrice")
boxplot(log10(train$SalePrice), main = "Log10(SalePrice)")

par(mfrow = c(1, 1))
```

- Distribución de los valores perdidos en función de la variable Log(SalePrice)

```{r} # TODO: DELETE ¿?
for (feature in names(filtered_lost_values_info)) {
    categories <- train[, feature]
    print(ggplot(data = train, aes(x = feature, y = log(SalePrice), fill = categories)) +
        geom_boxplot())
}
```

- Examinamos la distribución de las variables continuas con respecto a Log(SalePrice)

```{r}
lost.values.features <- rownames(getLostValuesStats())
numeric.features <- names(train)[which(sapply(train, is.numeric))]
lost.values.features.numeric <- dplyr::intersect(numeric.features, lost.values.features)

plots <- lapply(lost.values.features.numeric, function(feature) {
    ggplot(data = train, aes(x = train[, feature], y = log(train$SalePrice))) +
          geom_point() +
          geom_smooth(method = "lm") +
          xlab(label = feature) +
          ylab(label = "Log(SalePrice)")
})

cowplot::plot_grid(plotlist = plots, ncol = 3)
```

# Tratamiento de los valores perdidos

- GarageYrBlt

Se aprecia que es una propidad que, lógicamente, está muy relacionada con YearBuilt (año de construcción). En general, se puede decir que GarageYrBlt tiende a ser igual a YearBuilt. Por consiguiente, en los valores perdidos de GarageYrBlt, se procede a asígnar el correspondiente valor de YearBuilt.

```{r}
ggplot(data = full.set, aes(x = GarageYrBlt, y = YearBuilt)) +
          geom_point() +
          geom_smooth(method = "lm")

full.set$GarageYrBlt[full.set$GarageYrBlt == 2207] <- 2007
full.set$GarageYrBlt[is.na(full.set$GarageYrBlt)] <- full.set$YearBuilt[is.na(full.set$GarageYrBlt)]

updatePartitions()
```

- LotFrontage

Por lógica, se puede decir que el área de la propiedad con la longitud de la fachada. Para confirmarlo, comprobamos la correlación entre ellas:

```{r}
cor(full.set$LotFrontage, full.set$LotArea, use = "complete.obs")
cor(log(full.set$LotFrontage), log(full.set$LotArea), use = "complete.obs")
```

Y visualizamos su relación:

```{r}
plotLotRelation <- ggplot(data = full.set, aes(x = LotArea, y = LotFrontage)) +
    geom_point() +
    geom_smooth(method = "lm")

plotLogLotRelation <- ggplot(data = full.set, aes(x = log(LotArea), y = log(LotFrontage))) +
          geom_point() +
          geom_smooth(method = "lm")

cowplot::plot_grid(plotLotRelation, plotLogLotRelation, ncol = 2)
```

Se puede confirmar que existe una alta correlación directa entre *LotFrontage* con *LotArea*. Dado, que estas dos propiedades están relacionadas, seguramente una de ellas sea desechada en el proceso de selección de variables. Idependientemente de ello, en este paso sustituiremos los valores de *LotFrontage*, por la mediana de los valores existentes.

```{r}
full.set$LotFrontage[is.na(full.set$LotFrontage)] <- mean(full.set$LotFrontage[!is.na(full.set$LotFrontage)])

updatePartitions()
```

```{r}
cor(full.set$LotFrontage, full.set$LotArea, use = "complete.obs")
cor(log(full.set$LotFrontage), log(full.set$LotArea), use = "complete.obs")
```

Observamos que la correlación continúa siendo similar después de tratar los valores perdidos en *LotFrontage*.

**TODO:** Quizás remplazar con la media no sea la mejor opción (cambia bastante la correlación). Si no se encuentra una solución mejor, quizás habría que cargarse directamente la variable.

```{r}
ggplot(data = full.set, aes(x = log(LotArea), y = log(LotFrontage))) +
          geom_point() +
          geom_smooth(method = "lm")
```

- MasVnrArea

Existe una gran cantidad de entradas con valor 0. Esto seguramente se deba a la carencia de "chapado":

```{r}
ggplot2::qplot(data = train, x = log(MasVnrArea), y = log(SalePrice), col = MasVnrType)
```

También observamos que en ambas variables los valores perdidos (8) forman parte de los mismos ejemplos:

```{r}
full.set$Id[is.na(full.set$MasVnrArea)]
full.set$Id[is.na(full.set$MasVnrType)]
```

# TODO: hay una entrada más en el conjunto de test
Por consiguiente, se elimina la caraterística *MasVnrArea*, ya que las entradas con valor 0, por ser del tipo "None", hacen que la información desprendida de la variable esté "deformada".

```{r}
full.set <- dplyr::select(full.set, - MasVnrArea)

updatePartitions()
```

Ahora se deben tratar los valores perdidos de *MasVnrType*. Para ello, observamos *MasVnrType* en relación a *SalePrice* para entender su distribución.

```{r}
qplot(data = train, x = MasVnrType, y = log10(SalePrice), geom = c("boxplot"), fill = MasVnrType)
```

Asignamos a los valores perdidos el tipo "BrkFace", por mayor proximidad de sus medias. Aunque también se les podría asignar el tipo "Stone".

```{r}
full.set$MasVnrType[is.na(full.set$MasVnrType)] <- "BrkFace"

updatePartitions()
```

Finalmente, observamos la distribución resultante en las categorías de *MasVnrType* en relación a *SalePrice*.

```{r}
qplot(data = train, x = train$MasVnrType, y = log10(train$SalePrice), geom = c("boxplot"), fill = train$MasVnrType)
```

## Variables categóricas

Las siguientes caraterísticas con valores perdidos se corresponden con aquellas que contienen entradas en las que hay una ausencia de la propiedad a la que representan. Por ejemplo, la propiedad *PoolQC* representa la calidad de la piscina, pero es obvio que en aquellas propiedades en las haya una ausencia de piscina, será inviable representar su calidad. Por consiguiente, se le asignarán el tipo "None" a aquellos valores ausentes (NA). Las caracteríasticas que contienen este tipo de valores perdidos son:  *PoolQC*,   *MiscFeature*, *Alley*,  *Fence*, *FireplaceQu*, *GarageCond*, *GarageFinish*, *GarageQual*, *GarageType*, *BsmtCond*, *BsmtExposure*, *BsmtFinType1*, *BsmtFinType2* y *BsmtQual*.

```{r}
none_types <- c(
  "PoolQC",
  "MiscFeature",
  "Alley",
  "Fence",
  "FireplaceQu",
  "GarageCond",
  "GarageFinish",
  "GarageQual",
  "GarageType",
  "BsmtCond",
  "BsmtExposure",
  "BsmtFinType1",
  "BsmtFinType2",
  "BsmtQual"
  )

for (none in none_types) {
    if (is.factor(full.set[, none])) {
        full.set[, none] <- as.character(full.set[, none])
        full.set[, none][which(is.na(full.set[, none]))] <- "None"
        full.set[, none] <- factor(full.set[, none])
    }
}

updatePartitions()
```

- Electrical

Esta característica presenta un valor perdido. Dada la mínima influencia que puede tener, se le asigna la categoría mayoritaria.

```{r}
full.set$Electrical[is.na(full.set$Electrical)] <- as.character(sort(full.set$Electrical, decreasing = TRUE)[1])

updatePartitions()
```

# Corrección de valores perdidos en el conjunto de test

En primer lugar, se examinan los valores perdidos que presenta el conjunto de test y se visualizan de la misma forma que se procedió con el conjunto de entrenamiento.

```{r}
getLostValuesStats()
```

# TODO: ¿En las caraterísticas que presenten valores perdidos y ya se haya examinado previamente, se proceden a tratar de la misma forma para realizar un procedimiento consistente?
Así mismo, se procede a tratar las demás variables. Por una parte, se procesan las de tipo numérico, asignando la mediana a los valores faltantes.

```{r}
lost.test.numeric.features <- c("BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath", "GarageCars", "GarageArea")

for (feature in lost.test.numeric.features) {
    full.set[, feature][is.na(full.set[, feature])] <- median(full.set[, feature][!is.na(full.set[, feature])])
}

updatePartitions()
```

En cuanto a las variables de tipo nominal, se les asigna la moda de sus valores.

```{r}
lost.test.categorical.features <- c("Exterior1st", "Exterior2nd", "Functional", "KitchenQual", "MSZoning", "SaleType", "Utilities")

for (feature in lost.test.categorical.features) {
    full.set[, feature][is.na(full.set[, feature])] <- as.character(sort(full.set[, feature], decreasing = T)[1])
}

updatePartitions()
```

Antes de continuar, comprobamos que no hay valores perdidos en ninguno de los dos conjuntos.

```{r}
getLostValuesStats()
```

# Transformación de datos

En primer lugar, se procede a eliminar la propiedad Id de los conjuntos de entrenamiento y test.

```{r}
train.transformed <- dplyr::select(train, - Id)
test.transformed <- dplyr::select(test, - Id)
```

# Tratamiento de outliers

```{r}
plot.with.outliers <- ggplot(train.transformed, aes(y = SalePrice, x = GrLivArea)) + ggtitle("With Outliers") + geom_point()
plot.with.outliers.log <- ggplot(train.transformed, aes(y = log(SalePrice), x = log(GrLivArea))) + ggtitle("With Outliers (Log())") + geom_point()
cowplot::plot_grid(plot.with.outliers, plot.with.outliers.log, ncol = 2)

train.transformed[train.transformed$GrLivArea > 4000,]$GrLivArea <- mean(train.transformed$GrLivArea) %>% as.numeric

ggplot(train.transformed, aes(y = SalePrice, x = GrLivArea)) + ggtitle("Without Outliers") + geom_point()
```


```{r}
full.set.transformed <- data.frame(data.table::rbindlist(list(train.transformed, test.transformed), use.names = F, fill = F))
```

- Feature engeneriing

Creación de una nueva variable: Area total basement e

```{r}
full.set.transformed <- dplyr::select(full.set.transformed, - LotFrontage)

full.set.transformed$TotalSF = full.set.transformed$TotalBsmtSF + full.set.transformed$X1stFlrSF + full.set.transformed$X2ndFlrSF

full.set.transformed$Age <- full.set.transformed$YrSold - full.set.transformed$YearRemodAdd

full.set.transformed$TotalProch <- full.set.transformed$EnclosedPorch + full.set.transformed$ScreenPorch + full.set.transformed$X3SsnPorch
```

## Regularización de las variables continuas

Tal y como se ha mostrado, la variable *SalePrice* contiene una distribución asimétrica. Por consiguiente, para evitar el efecto que los valores extremos puedan causar, se procede a aplicar logaritmos a los valores de la distribución. 

Así mismo, se procede a mostrar diagramas de densidad de cada una de las características que contengan datos númericos. De esta forma se podrá observa que transformaciones pueden ser convenientes de hacer a cada variable.

```{r}
full.set.transformed$SalePrice <- log(full.set.transformed$SalePrice)
```

```{r}
continuous.features <- c(
  "LotArea", ## Lot size in square feet
  "BsmtFinSF1", ## Type 1 finished square feet    
  "BsmtFinSF2", ## Type 2 finished square feet
  "BsmtUnfSF", ## Unfinished square feet of basement area
  "TotalBsmtSF", ## Total square feet of basement area
  "X1stFlrSF", ## First Floor square feet
  "X2ndFlrSF", ## Second floor square feet
  "LowQualFinSF", ## Low quality finished square feet (all floors)
  "GrLivArea", ## Above grade (ground) living area square feet
  "GarageArea", ## Size of garage in square feet
  "WoodDeckSF", ## Wood deck area in square feet
  "OpenPorchSF", ## Open porch area in square feet  
  "EnclosedPorch", ## Enclosed porch area in square feet 
  "X3SsnPorch", ## Three season porch area in square feet 
  "ScreenPorch", ## Screen porch area in square feet
  "PoolArea" ## Pool area in square feet
)

plots <- lapply(continuous.features, function(feature) {
    if (is.numeric(full.set.transformed[, feature])) {
        ggplot2::ggplot(data = full.set.transformed, aes(x = full.set.transformed[, feature])) +
                      geom_density() +
                      xlab(feature)
    }
})

cowplot::plot_grid(plotlist = plots, ncol = 3)
```

Let's normalize the continuous values

```{r}
full.set.transformed[, continuous.features] <- log(1 + full.set.transformed[, continuous.features])

plots <- lapply(continuous.features, function(feature) {
    if (is.numeric(full.set.transformed[, feature])) {
        ggplot2::ggplot(data = full.set.transformed, aes(x = full.set.transformed[, feature])) +
                      geom_density() +
                      xlab(feature)
    }
})

cowplot::plot_grid(plotlist = plots, ncol = 3)
```

Center & scale....

```{r}
full.set.preProcessed <- caret::preProcess(select(full.set.transformed, - SalePrice), method = c("center", "scale"))
full.set.transformed <- predict(full.set.preProcessed, full.set.transformed)
```

Cambiado las variable categóricas por numéricas
# TODO: Buscar explicación

```{r}
for (i in 1:ncol(full.set.transformed)) {
    if (is.factor(full.set.transformed[, i])) {
        levels(full.set.transformed[, i]) <- c(1:length(levels(full.set.transformed[, i])))
        full.set.transformed[, i] <- as.numeric(full.set.transformed[, i])
    }
}
```

```{r}
full.set.transformed.dummy <- dummy.data.frame(full.set.transformed, dummy.classes = "character")

features.class <- sapply(names(full.set.transformed.dummy), function(feature) {
    class(full.set.transformed.dummy[[feature]])
})
features.class.numeric <- names(features.class[features.class != "character"])

features.class.numeric.skewed <- sapply(features.class.numeric, function(feature) {
    e1071::skewness(full.set.transformed.dummy[[feature]], na.rm = TRUE)
})
features.class.numeric.skewed <- features.class.numeric.skewed[abs(features.class.numeric.skewed) > 0.75]

for (feature in names(features.class.numeric.skewed)) {
    box.cox = caret::BoxCoxTrans(full.set.transformed.dummy[[feature]], lambda = 0.15)
    full.set.transformed.dummy[[feature]] = predict(box.cox, full.set.transformed.dummy[[feature]])
}
```

- Spliting into train and test

```{r}
train.processed <- full.set.transformed.dummy[1:nrow(train.transformed),]
test.processed <- full.set.transformed.dummy[(nrow(train.transformed) + 1):nrow(full.set.transformed.dummy),]
test.processed <- dplyr::select(test.processed, - SalePrice)
```

- lm.....

```{r}
exploratory.lm = lm(SalePrice ~ ., data = train.processed)

par(mfrow = c(2, 2))
plot(exploratory.lm)
par(mfrow = c(1, 1))
```

```{r}
importance <- caret::varImp(exploratory.lm)
importance.sort <- sort(importance$Overall, decreasing = TRUE, index.return = TRUE)

data.frame("Feature" = rownames(importance)[importance.sort$ix], "Overall" = importance[importance.sort$ix,])[1:15, ]
```

## Entrenamiento

- Entrenamiento "parcial"

```{r}
train.processed.partition.index <- createDataPartition(train.processed$SalePrice, p = 0.7, list = FALSE)
train.processed.partition.train <- train.processed[train.processed.partition.index,]
train.processed.partition.validation <- train.processed[-train.processed.partition.index,]
```

```{r}
model.partial <- caret::train(SalePrice ~ .,
  data = train.processed.partition.train,
  method = 'gbm',
  trControl = caret::trainControl(
  method = 'repeatedcv',
  number = 10,
  repeats = 1
  ))

model.partial
```

- Validación del modelo

```{r}
predictAndEvaluate <- function(model, validation.set) {
    validation.prediction <- stats::predict(model, dplyr::select(validation.set, - SalePrice))

    print(ggplot2::qplot(x = validation.prediction, y = validation.set$SalePrice,
               geom = c("point", "smooth"), method = "lm",
               xlab = "Predicted", ylab = "Real"))

    rmse(validation.set$SalePrice, validation.prediction)
}
```

```{r}
predictAndEvaluate(model.partial, train.processed.partition.validation) # 0.134816
```

Trying to improve model ...

```{r}
ensembleTrain <- function(train.set) {
    trControl <- trainControl(
        method = "cv",
        number = 7,
        savePredictions = "final",
        index = createResample(train.set$OverallQual, 7),
        allowParallel = TRUE
    )

    modelList <- caretEnsemble::caretList(
                  SalePrice ~ .,
                  data = train.set,
                  trControl = trControl,
                  metric = "RMSE",
                  tuneList = list(
    gbm = caretModelSpec(method = "gbm", tuneGrid = expand.grid(n.trees = 700, interaction.depth = 5,
                                                          shrinkage = 0.05, n.minobsinnode = 10)),
    xgbTree = caretModelSpec(method = "xgbTree", tuneGrid = expand.grid(nrounds = 2500, max_depth = 6, min_child_weight = 1.41,
                                                                  eta = 0.01, gamma = 0.0468, subsample = 0.769,
                                                                  colsample_bytree = 0.283))
    ))

    greedy_ensemble <- caretEnsemble(modelList, metric = "RMSE", trControl = trainControl(number = 25))

    return(greedy_ensemble)
}
```

```{r}
model.ensemble <- ensembleTrain(train.processed.partition.train)
```

```{r}
model.ensemble # 0.1322569
predictAndEvaluate(model.ensemble, train.processed.partition.validation) # 0.1230385
```

## Full train

```{r}
model.full <- ensembleTrain(train.processed)
model.full
```

## Prediction and submit

```{r}
predictions <- predict(model.full, newdata = test.processed)
prediction.table <- data.frame(Id = test$Id, SalePrice = exp(predictions))
write.csv(prediction.table, "clean_prediction_3.csv", row.names = FALSE)
```

